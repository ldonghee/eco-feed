package com.dhlee.batch.job

import com.dhlee.core.domain.article.NewsArticle
import com.dhlee.core.domain.parser.YonhapNewsParser
import io.github.oshai.kotlinlogging.KotlinLogging
import kotlinx.coroutines.coroutineScope
import kotlinx.coroutines.delay
import kotlinx.coroutines.flow.*
import org.springframework.stereotype.Component
import org.springframework.web.reactive.function.client.WebClient
import org.springframework.web.reactive.function.client.awaitBody
import org.springframework.web.reactive.function.client.awaitBodyOrNull

@Component
class YonhapNewsCrawler(
  private val webClient: WebClient,
  private val parser: YonhapNewsParser
) {
  private val logger = KotlinLogging.logger {}

  companion object {
    private const val BASE_URL = "https://www.yna.co.kr"
    private const val ECONOMY_URL = "$BASE_URL/economy/all"
    private const val MAX_PAGES = 3
    private const val MAX_CONCURRENT = 10
    private const val REQUEST_DELAY_MS = 100L
  }

  /**
   * ì—°í•©ë‰´ìŠ¤ ê²½ì œ ì„¹ì…˜ í¬ë¡¤ë§
   */
  suspend fun crawlEconomyNews(): List<NewsArticle> = coroutineScope {
    logger.info { "=== ì—°í•©ë‰´ìŠ¤ ê²½ì œ í¬ë¡¤ë§ ì‹œì‘ ===" }

    try {
      // 1ë‹¨ê³„: ê¸°ì‚¬ URL ìˆ˜ì§‘
      val articleUrls = collectArticleUrls()
      logger.info { "ğŸ“‹ ìˆ˜ì§‘ëœ ê¸°ì‚¬ URL: ${articleUrls.size}ê°œ" }

      if (articleUrls.isEmpty()) {
        logger.warn { "ìˆ˜ì§‘ëœ URLì´ ì—†ìŠµë‹ˆë‹¤" }
        return@coroutineScope emptyList()
      }

      // 2ë‹¨ê³„: ê° ê¸°ì‚¬ í¬ë¡¤ë§ (ë³‘ë ¬ ì²˜ë¦¬)
      val articles = articleUrls.asFlow()
        .buffer(MAX_CONCURRENT) // ë™ì‹œ ì²˜ë¦¬ ì œí•œ
        .map { url ->
          delay(REQUEST_DELAY_MS) // ì„œë²„ ë¶€í•˜ ë°©ì§€
          crawlArticleDetail(url)
        }
        .filterNotNull()
        .toList()

      logger.info { "âœ… í¬ë¡¤ë§ ì™„ë£Œ: ${articles.size}ê°œ ê¸°ì‚¬" }

      // 3ë‹¨ê³„: ê²°ê³¼ ë¡œê¹…
      logCrawlingResults(articles)

      articles
    } catch (e: Exception) {
      logger.error(e) { "í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ" }
      emptyList()
    }
  }

  /**
   * ëª©ë¡ í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ URL ìˆ˜ì§‘
   */
  private suspend fun collectArticleUrls(): Set<String> {
    val urls = mutableSetOf<String>()

    repeat(MAX_PAGES) { page ->
      try {
        val pageUrl = if (page == 0) ECONOMY_URL else "$ECONOMY_URL/$page"

        logger.info { "ğŸ“„ ëª©ë¡ í˜ì´ì§€ ${page + 1} í¬ë¡¤ë§: $pageUrl" }

        val html = webClient.get()
          .uri(pageUrl)
          .retrieve()
          .awaitBodyOrNull<String>()

        if (html != null) {
          val pageUrls = parser.parseArticleLinks(html)
          urls.addAll(pageUrls)
          logger.info { "   âœ ë°œê²¬ëœ ë§í¬: ${pageUrls.size}ê°œ" }
        }

        delay(REQUEST_DELAY_MS)
      } catch (e: Exception) {
        logger.error(e) { "í˜ì´ì§€ ${page + 1} ìˆ˜ì§‘ ì‹¤íŒ¨" }
      }
    }

    return urls
  }

  /**
   * ê¸°ì‚¬ ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§
   */
  private suspend fun crawlArticleDetail(url: String): NewsArticle? {
    return try {
      logger.debug { "ğŸ“° ê¸°ì‚¬ í¬ë¡¤ë§: $url" }

      val html = webClient.get()
        .uri(url)
        .retrieve()
        .awaitBody<String>()

      parser.parseArticleDetail(url, html)?.also {
        logger.debug { "   âœ… íŒŒì‹± ì™„ë£Œ: ${it.title}" }
      }
    } catch (e: Exception) {
      logger.error(e) { "ê¸°ì‚¬ í¬ë¡¤ë§ ì‹¤íŒ¨: $url" }
      null
    }
  }

  /**
   * í¬ë¡¤ë§ ê²°ê³¼ ë¡œê¹…
   */
  private fun logCrawlingResults(articles: List<NewsArticle>) {
    logger.info { "\n" + "=".repeat(80) }
    logger.info { "ğŸ“Š í¬ë¡¤ë§ ê²°ê³¼ ìš”ì•½" }
    logger.info { "=".repeat(80) }
    logger.info { "ì´ ê¸°ì‚¬ ìˆ˜: ${articles.size}ê°œ" }
    logger.info { "" }

    articles.forEachIndexed { index, article ->
      logger.info {
        """
                [${index + 1}] ${article.title}
                   URL: ${article.url}
                   ë°œí–‰: ${article.publishedAt ?: "ì•Œ ìˆ˜ ì—†ìŒ"}
                   ê¸°ì: ${article.author ?: "ì•Œ ìˆ˜ ì—†ìŒ"}
                   ì¸ë„¤ì¼: ${article.thumbnailUrl ?: "ì—†ìŒ"}
                   ë³¸ë¬¸ ê¸¸ì´: ${article.content?.length ?: 0} ì
                """.trimIndent()
      }
      logger.info { "-".repeat(80) }
    }

    logger.info { "=".repeat(80) }
  }
}